---
title: "Essential Statistics"
subtitle: "PhD+ 2023: Data Literacy in R<br>2023-03-29"
author: "Clay Ford, UVA Library"
format:
  revealjs:
    embed-resources: true
    smaller: true
    scrollable: true
    slide-number: true
    logo: ../images/logo.png
    footer: clayford@virginia.edu | GitHub [@clayford](https://github.com/clayford)
---

## Data Literacy

```{r echo=FALSE}
d <- readRDS("../data/albemarle_homes_2023.rds")
# d <- readRDS("data/albemarle_homes_2023.rds")
```

> data literacy...describes the ability to not only carry out statistical analysis on real world problems, but also to understand and critique any conclusions drawn by others on the basis of statistics.

David Spiegelhalter, *The Art of Statistics*

## Agenda

In this session we explore using R to carry out and interpret basic statistical analyses and highlight potential sources of statistical errors.

-   Counts and Proportions
-   Summarizing Numeric Data
-   Uncertainty
-   Hypothesis testing 
-   Modeling

## Hold the bacon?

In 2015 the World Health Organization (WHO) [announced](https://www.who.int/news-room/questions-and-answers/item/cancer-carcinogenicity-of-the-consumption-of-red-meat-and-processed-meat) that eating 50g of processed meat a day was associated with an increased risk of bowel cancer of 18%.

How concerned should we be?

It depends. Is that an *absolute* increase or a *relative* increase?

## Absolute versus Relative

-   Absolute is additive.
    -   2% + 18% = 20% (increase 18%)
    -   20% + -18% = 2% (decrease 18%)
-   Relative is multiplicative.
    -   2% x (1 + 0.18) 1.18 = 2.36% (increase 18%)
    -   20% x (1 - 0.18) 0.82 = 16.4% (decrease 18%)

The 18% increase mentioned in the WHO study is *relative*.

If the risk (probability) of bowel cancer without consuming 50g of processed meat per day is 0.001, then an 18% increase raises the risk to 0.001 x 1.18 = 0.00118.

::: callout-important
Interpreting a relative increase as absolute can make it seem more important than it really is.
:::

## Determining change

Given before and after proportions/percents...

-   use subtraction to determine **absolute** change
-   division (or a ratio) to determine **relative** change

Example: In 2014, 0.0694, or 6.94%, of U.S. households had at least one motorcycle. In 2018, that figure rose to 0.0802, or 8.02%. [source](https://www.prnewswire.com/news-releases/us-households-with-a-motorcycle-climbs-to-record-8-percent-in-2018-300783120.html)

**Absolute change**:\
Ownership increased by 8.02% - 6.94% = 1.08%\
The percentage of households increased from 6.94% to 8.02%

**Relative change**:\
Ownership increased by 8.02/6.94 = 1.1556 (or 15.56%)  
The 2014 percentage increased by 15.56%

## Proportions versus Percents

Proportions range from 0 to 1. (eg, 0.18)

Percents are proportions multiplied by 100 with a % sign appended to the value. (0.18 x 100 = 18%)

Most any statistical program is going to return *proportions*.

::: callout-important
Beware of confusing proportions with percents. Example: confusing a percent risk of 0.5% as a proportion (probability) of 0.5 (instead of 0.005)
:::

::: callout-important
Beware of importing data with percents into R. The percent symbols can result in numbers being treated as character data.
:::

## Don't forget the denominator

When used to summarize data, Percents and Proportions are both relative to the size of the data, which is in the denominator.

Example: 0.14, or 14%, of students surveyed said they were cancelling Netflix.

That could be 1/7, or 100/700. The former seems less important because of the small denominator.

::: callout-important
Beware of percents and proportions presented without the denominator (sample size) used to calculate them.
:::


::: callout-important
Also consider the source of the denominator. Was the survey random or administered in the dining hall during breakfast? 
:::

## Probability versus Odds

Probabilities (p) range from 0 to 1. Proportions are often interpreted as estimates of probabilities. 

Odds = p/(1 - p) and range from 0 to infinity.

If the probability of rain tomorrow is 0.2, the odds are 0.2/(1 - 0.2) = 1/4.

If the probability of rain tomorrow is 0.8, the odds are 0.8/(1 - 0.8) = 4.

::: callout-important
Beware of expressing or interpreting probabilities as odds and vice versa.
:::

Let's go to R.



## Our brains can't handle this

```{r echo=TRUE}
head(d$totalvalue, n = 200)
```

## Center and Spread

We usually summarize numeric data by calculating some measure of the center and the spread.

-   *Center* is usually interpreted as a typical or representative value. That's not always correct.
-   *Spread* is usually interpreted as how far away we can expect any value to be from the center value. That's not always useful.

The most common measures of center are the **mean** and **median**.

The most common measures of spread are the **standard deviation (SD)** and **interquartile range (IQR)**. 

## Mean and Median

The mean is the point at which the data would balance on a fulcrum.

The median is the middle point of data sorted in ascending order.

When data are *symmetric*, the mean and median are close.

When data are *skewed*, the median may be a better measure of center or typical value.

::: callout-tip
You can examine and present both means and medians. You don't have to pick one.
:::

::: callout-important
Means and medians as summary measures are not always the best estimates of the expected or most representative value.
:::

## Some distributions

[Mean]{style="color:red;"} and [Median]{style="color:blue;"}

```{r}
f <- function(x){
  # points(x = mean(x), y = -5, pch = 17, col = "red")
  abline(v = mean(x), col = "red")
  # points(x = median(x), y = -5, pch = 17, col = "blue")
  abline(v = median(x), col = "blue")
}
set.seed(2)
n <- 500
y1 <- rnorm(n)
y2 <- rexp(n)
x <- sample(0:1, replace = T, size = n)
y3 <- ifelse(x == 1, rnorm(n,5,1), rnorm(n,11,2))
y4 <- runif(n)

op <- par(mfrow = c(2,2), mar = c(2, 4, 2, 2) + 0.1)
hist(y1, main = "symmetric\nmean/median representative", 
     xlab = "", ylab = "", cex.main = 0.9)
f(x = y1)
hist(y2, main = "skewed\nmean/median somewhat representative", 
     xlab = "", ylab = "", cex.main = 0.9)
f(x = y2)
hist(y3, main = "bimodal\nmean/median not representative", 
     xlab = "", ylab = "", cex.main = 0.9)
f(x = y3)
hist(y4, main = "symmetric\nmean/median not representative", 
     xlab = "", ylab = "", cex.main = 0.9)
f(x = y4)
par(op)
```


## Standard Deviation and IQR

Standard Deviation is basically how far a given value is expected to be from the mean. It's really only appropriate for symmetric data. Like the mean, it can be easily influenced by extreme values.

IQR is the difference between the 75th and 25th percentiles. It contains the middle 50% of the data. It is unaffected by extreme values.

```{r echo=TRUE}
sd(d$totalvalue)
IQR(d$totalvalue)
```

As summaries of data, they can be confusing if your audience is not familiar with the concept of spread.

::: callout-tip
Data with no variability has a standard deviation/IQR of 0. SD/IQR is $\ge 0$
:::

## Percentiles/Quantiles/Quartiles

Observe the summary output for Albemarle Real Estate home values:

```{r echo=TRUE}
summary(d$totalvalue)
```

The value labeled "1st Qu.", `r scales::dollar(as.integer(quantile(d$totalvalue, probs = 0.25)))`, is the 1st quartile, 0.25 quantile, and 25th percentile. 

- 1st quartile = the value below which lies one quarter (1/4) of the data
- 0.25 quantile = the value below which lies 0.25 of data
- 25th percentile = the value below which lies 25% of data

The median is the 2nd quartile, 0.5 quantile, and 50th percentile. "3rd Qu." is the 3rd quartile, 0.75 quantile, and 75th percentile.

The `quantile()` function returns the quantile/percentile you specify. 

## Empirical Cumulative Distributions

The output below says homes valued at \$855,430 are in the 90th percentile of homes. 90% of homes are valued lower.

```{r echo=TRUE}
quantile(d$totalvalue, probs = 0.90)
```

What percentile is a home valued at \$350,000 in? To answer this we need to determine the _Empirical cumulative distribution_. R makes this relatively easy with the `ecdf()` function.

The `ecdf()` function actually creates a function from your data. You then use that function to get the quantile/percentile of a given data point.

```{r echo=TRUE}
Fn <- ecdf(d$totalvalue)
Fn(350000)
```

A home valued at \$350,000 is in about the 37th percentile.

## Transforming data

Transforming data to a different scale can reduce the impact of extreme values. A common transformation is the _log_ transformation.

This basically translates data into _magnitudes_.

```{r echo=TRUE}
x <- c(10, 100, 1000, 10000, 100000, 1000000)
log10(x) # what power do we raise 10 to get x? Log base 10
log(x) # what power do we raise e to get x? Natural log
```

Natural logs are often preferred in statistical analyses because they can make interpretation easier. 

They are also appropriate when large data values can be assumed to be bigger in a _relative_ sense rather than an _absolute_ sense. (Managers earn 20% more than workers. Directors earn 30% more than managers. Executives earn 40% more than Directors. And so on.)

## Before and After

Notice how total value goes from ranging from 4200 - 8,000,000 to about 10 - 16 after a log transformation.

```{r}
op <- par(mfrow = c(1,2), pty = "s")
hist(d$totalvalue)
hist(log(d$totalvalue))
par(op)
```

::: callout-tip
Log transformations only work for positive data.
:::

## Summarizing two sets of values

Below is a scatterplot of 200 randomly sampled homes from Albemarle County comparing total value to finished sq ft. How can we summarize this relationship?

```{r}
set.seed(23)
i <- sample(nrow(d), size = 200)
samp_d <- d[i,]
plot(totalvalue ~ finsqft, data = samp_d)
```

## Correlation

Correlation is a single value that can (sometimes) summarize a relationship between two sets of numeric data.

Correlation is unitless and ranges from -1 to 1. It expresses how close dots fall along a line. [See this useful Wikipedia illustration](https://en.wikipedia.org/wiki/Correlation_and_dependence#/media/File:Correlation_examples2.svg)

**Pearson** correlation measures linear association.\
**Spearman** *rank* correlation can measure non-linear association such as curved lines.

The correlation of the data in the previous plot can be calculated as follows:

```{r echo=TRUE}
cor(samp_d$finsqft, samp_d$totalvalue, method = "pearson")
cor(samp_d$finsqft, samp_d$totalvalue, method = "spearman")
```

## Correlation warning

Correlations near 0 do not mean there is no relationship. Observe the following, courtesy of [The Datasaurus Dozen](https://github.com/jumpingrivers/datasauRus). Lesson: plot your data. Let's go to R!

```{r}
library(datasauRus)
dat <- datasaurus_dozen_wide
op <- par(mfrow = c(1,2), pty = "s")
plot(slant_down_y ~ slant_down_x, data = dat, xlab = "", ylab = "")
plot(dino_y ~ dino_x, data = dat, xlab = "", ylab = "")
par(op)
```

```{r echo=TRUE}
cor(dat$slant_down_x, dat$slant_down_y) # slanting lines
cor(dat$dino_x, dat$dino_y)  # t-rex

```



## Trees

> "we estimate that the global number of trees is approximately 3.04 trillion (+/-0.096 trillion, 95% confidence interval (CI))"

Mapping tree density at a global scale, [Nature](https://www.nature.com/articles/nature14967) (2015)

## Uncertainty

Sometimes we can collect all the data in a population (eg, Albemarle County real estate data). The mean of the data is the mean. There is no uncertainty. 

Sometimes we can only collect a sample (eg, voters, trees, cows). Hopefully it's a _random sample_. The mean of the sample data is _not_ the mean of the entire population. It is an _estimate_.

We often use the estimate as our best guess at the population mean. How uncertain is this estimate?

We usually try to assess this with _standard errors_ and _confidence intervals_.

## Standard Errors

Imagine randomly sampling 30 UVA students, weighing their backbacks, and taking the mean weight. And then sampling 30 more students and getting another mean. And again, and again, thousands of times, getting more means.

Now calculate the standard deviation of those means. That's the _standard error_.

The standard deviation of the sample means summarizes the uncertainty of our sample mean.

In practice, we don't take thousands of means. We take one and we usually calculate standard error using a formula such as $SD_{\text{sample}}/\sqrt{n}$.

For example, a [study](https://www.sciencedirect.com/science/article/pii/S2314853516300555) estimated the mean weight of chicken eggs to be about 55.02g with a standard error of 0.40 when the chicken was 25 weeks old. 

::: callout-important
Be cautious of interpreting estimates presented without standard errors.  
:::

## The Central Limit Theorem

Previously we saw distributions of _raw data_ for which the standard deviation was not a good measure of spread.

However it turns out the distribution of sample means calculated from most distributions is fairly symmetric. This is the _Central Limit Theorem_. This means we can be fairly confident about standard error estimates.

```{r}
set.seed(1234)
means <- replicate(n = 1000, expr = {
  mean(rchisq(n = 30, df = 8))
})

op <- par(mfrow = c(1,2), pty = "s")
curve(dchisq(x, df = 8), from = 0, to = 30, ylab = "", xlab = "", 
      main = "skewed population\nmean = 8")
plot(density(means), ylab = "", xlab = "", main = "distribution of 1000 sample means\nfrom skewed population (n=30)")
par(op)
```



## Confidence Intervals

Adding and subtracting standard errors to our estimate forms an interval of what we might say is a reasonable range that contains the true population value.

The most common interval calculated is the _95% confidence interval_ (CI).

We say "95%" because the method should work 95% of the time based on the number of standard errors we add/subtract to create the interval.

We call it "confidence" because we are confident in the method. 

## Confident in the process

If we were to take thousands of samples and calculate thousands of 95% confidence intervals, about 95% of the intervals should contain the true value. 

The 95% confidence interval on the number of trees (in trillions) was [2.94, 3.14]. That interval either contains the true number or it does not. 

::: callout-important
A confidence interval is _not_ a probability interval. Do not say there is a 95% probability the value is between the lower and upper bounds.
:::

Let's go to R.

## Cow painting

> "The total numbers of biting flies on the legs and body for B&W cows were almost half those on Control (p < 0.0001)." [paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0223447)

Painting 'Zebra Stripes' on Cows Wards Off Biting Flies [source](https://www.realclearscience.com/quick_and_clear_science/2019/10/07/painting_zebra_stripes_on_cows_wards_off_biting_flies.html)

![](../images/zebra_cow.jpg){fig-alt="cow painted like zebra." fig-align="right"}

## Hypothesis testing and p-values

Let's say we believe a coin is fair and lands heads 50% of the time.

We flip it 20 times and get 18 heads. That's 18/20 = 0.90, or 90%.

If the coin was really fair, what is the probability of getting 18 _or more_ heads?

The answer is about 0.0002. That's a _p-value_.

That's a small probability and leads us to reject the belief that the coin is fair.

What we just described is an example of a _hypothesis test_.


## Hypothesis testing and p-values

Hypothesis testing estimates the probability of obtaining the data we collected, or more extreme data, under the assumption of some hypothesis (usually a null/no effect hypothesis).

What is the probability a cow painted like a zebra would have 55 _or fewer_ fly bites than an unpainted cow which had 128, _assuming the paint job had no effect_? The authors state p < 0.0001.

Historically, p-values < 0.05 have been interpreted as "statistically significant", though many prominent statisticians are [now calling to abolish that threshold and the phrase "statistically significant"](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913).

Nevertheless, p-values have been published, and will likely continue to be published, so it pays to know what they mean.

::: callout-tip
When you see a p-value, think "probability of the data, or more extreme data, assuming a null hypothesis was true."
:::


## p-values are slippery

P-values are often misinterpreted and misused.

1. They _do not_ measure the probability the null hypothesis is true or that the data were produced by random chance alone.
2. They _do not_ justify proof of something because they're less than 0.05.
3. They _do not_ prove the null hypothesis because they're greater than 0.05.
4. They _do not_ measure the size of an effect or the importance of a result.

See The American Statistical Association's [special issue](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913) on p-values. 

::: callout-important
P-values should not be revered as the final word in any analysis. 
::: 


## The Frequentist view

Hypothesis testing, p-values, and confidence intervals assume there is a _fixed but unknown true value_ we're trying to estimate. 

This is often called the **Frequentist** philosophy because p-values and confidence intervals summarize expected behavior in the _long run_.

**Example**: there is a true mean weight of student backpacks at UVA. Our sample and confidence interval is our attempt to estimate the range of the true mean. Repeated samples and confidence intervals will capture true mean 95% of the time.

**Example**: If painting cows like zebras has no effect on number of fly bites, we would expect to see the type of data we collected, or more extreme data, less than 0.0001 of the time.

## The Bayesian view

Another philosophical approach says forget estimating a value. Instead use expertise and common sense to hypothesize a _probability distribution_ for a value of interest.

We then collect data and update our _prior_ distribution to create a _posterior_ distribution.

We then use our posterior distribution to summarize the value of interest.

This is called the **Bayesian** philosophy, after a Presbyterian minister named Thomas Bayes.

## A quick Bayesian example

Perhaps we think the average backpack weighs around 18 lbs. A possible _prior distribution_ might be a Normal distribution with mean 18 and standard deviation of 5. (Fig 1)

After observing our data we use _Bayes' theorem_ to update our prior distribution to get a _posterior distribution_. (Fig 2)

We use our posterior distribution to calculate a _credible interval_. There's about 0.74 probability that the mean backpack weight is between 15.5 and 16.5 lbs. (Fig 3) 

```{r echo=FALSE}
#| label: fig-charts
#| fig-subcap: 
#|   - "Prior Distribution"
#|   - "Posterior Distribution"
#|   - "Credible Interval"
#| layout-ncol: 3
bm1 <- readRDS("../data/backpacks.rds")

# Fig 1
curve(dnorm(x, mean = 18, sd = 5), from = 3, to = 33, 
      ylab = "", xlab = "possible means")
# Fig 2
bm1.df <- as.data.frame(bm1)
plot(density(bm1.df$`(Intercept)`), ylab = "", main = "",
     xlab = "possible means", xlim = c(3,33))
# Fig 3
plot(density(bm1.df$`(Intercept)`), main = "", xlab = "possible means")
abline(v = 15.5, lty=2)
abline(v = 16.5, lty=2)
# text(x = 16.5, y = 0.7, labels = "P(15.5 < m < 16.5) = 0.74", pos = 4)

```


## The truth about statistics

> Students are frequently left with the impression that statistics is a monolith, firmly rooted in mathematics and thus somehow pure — while in truth the subject is highly contentious and as closely aligned to philosophy as it is to math.

Darren Dahly, [Statistical Reform](https://statsepi.substack.com/p/statistical-reform)

Let's go to R.

## Is education risky?

> Women with ≥3 years university education had increased risk of glioma (1.23, 1.08 to 1.40) and meningioma (1.16, 1.04 to 1.29) compared to those with primary education.

> Poisson regression models were used to calculate incidence rate ratios (IRR) for brain tumours by different indicators of socioeconomic position (SEP), with adjustment for potential confounders.

Socioeconomic position and the risk of brain tumour: a Swedish national population-based cohort study [source](https://jech.bmj.com/content/70/12/1222)



## Conditional means

The mean total value value of Albemarle homes is a little over \$500,000.

```{r echo=TRUE}
mean(d$totalvalue)
```

But the mean total value of Albemarle homes when they have 1,280 finished square is much lower.

```{r echo=TRUE}
mean(d$totalvalue[d$finsqft == 1280])
```

But the mean total value of Albemarle homes when they have 1,280 finished square and they have central air is a little higher.

```{r echo=TRUE}
mean(d$totalvalue[d$finsqft == 1280 & d$cooling == "Central Air"])
```

The expected value of a home appears to depend on certain features.

## Linear Models

A _linear model_ allows us to estimate expected values conditional on features or predictors.

A linear model is a function of predictors. For example, here's a simple model:

$$\text{total value} = 80,839 + 111\text{ finsqft} + 49,396 \text{ cooling}$$ 


This model estimates the mean expected value of homes with 2000 finished square feet and central air (cooling = 1) to be \$352,235 :

$$\text{total value} = 80,839 + 111(2000) + 49,396(1) = \$ 352,235$$ 

It says each additional finished square foot adds \$111 to the expected value of a home. It also says a home with central air is expected to be worth almost \$50,000 more than a home without central air. 

If we were interested in finsqft, we might say we have adjusted for the potential confounder cooling. 

## No model is perfect

A linear model attempts to _approximate_ the process that generated the data. It will always be wrong and uncertain, but hopefully it will be useful.

We pick the model. 

If we are satisfied with the model, we can use it to make predictions or summarize associations. 

This is also referred to as _Multiple Regression_ or _Regression Modeling_.

## Model uncertainty

Consider this model again:

$$\text{total value} = 80,839 + 111\text{ finsqft} + 49,396 \text{ cooling}$$ 

- The model may be bad. (Wrong predictors, left out predictors, not additive.)
- The _coefficients_ (80839, 111 and 49396) are just estimates. They're uncertain.
- A few homes may be unduly influencing how these coefficients are estimated.
- Predicted total values are uncertain.

## Model assumptions

Our model is approximate, not perfect. It's always going to be off by some random amount. Let's update it as follows:

$$\text{total value} = \color{blue}{80,839 + 111\text{ finsqft} + 49,396 \text{ cooling}} + \color{red}{\text{ERROR}}$$ 

Our model has two parts:

1. The [fixed]{style="color:blue;"} part
2. The [random error]{style="color:red;"} part

When we fit a model we have to make assumptions about the random errors. We assume they are random draws from a _probability distribution_.


## The dependent variable

The probability distribution for the random error _depends_ on what value we're modeling. 

- Home values might use a Normal distribution
- Incidents of brain tumors might use a Poisson distribution
- Defaulting on loan might use a Binomial distribution

These all have different names - linear model, count model, logistic regression - but they do the same thing: approximate the expected value of some variable given predictors of interest.

## Building linear models 

R is exceptionally good for building linear models. Perhaps too good.

> "A computer lets you make more mistakes faster than any other invention with the possible exceptions of handguns and Tequila." -Mitch Ratcliffe 

Building linear models in R is rather easy. Picking which predictors to include, specifying the model, and evaluating model performance is much harder.

::: callout-tip
Controlled paranoia is useful for building and evaluating linear models.
::: 

## It's all linear models

Many hypothesis tests are special cases of a linear model.

- 1-sample t-test: linear model with just an intercept
- 2-sample t-test: linear model with a binary predictor
- ANOVA: linear model with a categorical predictor
- 2-way ANOVA: linear model with two categorical predictors
- ANCOVA: linear model with a numeric predictor and categorical predictor

More information: [Common statistical tests are linear models (or: how to teach stats)](https://lindeloev.github.io/tests-as-linear/)

And of course all of this can be analyzed using Bayesian statistics. 

Let's go to R!

## Wrap up

This presentation is woefully deficient and incomplete. Essential statistics cannot be taught and learned in 2 hours.

But hopefully you've learned enough to respect the variability and uncertainty in statistics, and to be cautious and modest when drawing conclusions from statistics. 

> it is not necessary to remember all concepts and conversions, only to remember when to stop and enlist help. -Elisabeth Ribbans


## References

Spiegelhalter, David. (2019). _The Art of Statistics_. Basic Books, New York.

[Why I Don't Like Percents](https://www.fharrell.com/post/percent/)

[Statistics Notes: What is a percentage difference?](https://www.bmj.com/content/358/bmj.j3663)

[Statistical Thinking for the 21st Century](https://statsthinking21.github.io/statsthinking21-core-site/)

[Ten Simple Rules for Effective Statistical Practice](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004961)

[On the sixth day of Christmas – or was it the seventh? Why numbers matter more than ever](https://www.theguardian.com/commentisfree/2022/dec/29/on-the-sixth-day-of-christmas-or-was-it-the-seventh-why-numbers-matter-more-than-ever)

[Statistics Notes: The use of transformation when comparing two means](https://www.bmj.com/content/312/7039/1153)
