---
title: "PhD Plus: Essential Statistics"
author: "Clay Ford"
date: "Spring 2023"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---


Load packages we'll use today. In the first line, we only load two functions from the Hmisc package.

```{r}
library(Hmisc, include.only = c("smean.cl.normal", "smean.cl.boot"))
library(tidyverse)
library(ggeffects)
```

Read in the homes data from GitHub. The data is saved as an RDS file, which is the format for saving R objects. We need to use the `readRDS()` to read in the file. Notice we have to assign the result a name. I chose "d" because it's easy to type. Since the RDS file is on GitHub and accessed through a URL, we need to use the `url()` function to open the connection.

```{r}
URL <- "https://github.com/uvastatlab/phdplus2023/raw/main/data/albemarle_homes_2023.rds"
d <- readRDS(url(URL))
```

Before we get started, I want to drop the 5 home records that are not assigned to a school district. Below I select the three school district columns and apply the table function to those columns using the `map()` function which is from of the {purrr} package.

```{r}
d %>% 
  select(ends_with("district")) %>% 
  map(table)
```

It turns out these 5 homes are not assigned to any school district.

```{r}
d %>% 
  filter(hsdistrict == "Unassigned") %>% 
  select(ends_with("district")) 
  
```

I don't want to just drop the records. I also want to drop the Unassigned _level_ so it doesn't continue to appear in our analysis as having 0 records. For example, notice how the `table()` function shows Unassigned as 0 even after the call to `filter()`.


```{r}
d %>% 
  filter(hsdistrict != "Unassigned") %>% 
  select(ends_with("district")) %>% 
  map(table)

```

We can use the `droplevels()` function to drop levels with 0 counts.

```{r}
d <- d %>% 
  filter(hsdistrict != "Unassigned") %>%
  mutate(across(ends_with("district"), droplevels))

# a little more verbose but easier to read
# d <- d %>% 
#   filter(hsdistrict != "Unassigned") %>%
#   mutate(hsdistrict = droplevels(hsdistrict),
#          msdistrict = droplevels(msdistrict),
#          esdistrict = droplevels(esdistrict))
```

Now our tables show no level for "Unassigned"

```{r}
d %>% 
  select(hsdistrict, msdistrict, esdistrict) %>% 
  map(table)
```


## Counts and proportions

Simple counts of one variable can be obtained with `table()`

```{r}
table(d$hsdistrict)
```

To calculate proportions, we can pipe into `proportions()`

```{r}
table(d$hsdistrict) %>% proportions()
```


We can also use `xtabs()` function that allows us to use R's formula syntax. The syntax `~ hsdistrict` means count up by hsdistrict. Notice it includes the name of the variable in the output.

```{r}
xtabs( ~ hsdistrict, data = d)
xtabs( ~ hsdistrict, data = d) %>% proportions()
```


### 2-way tables

Create a cross-tabulation using `xtabs()`
`~ row_variable + column_variable`

Cross-tabulation of hsdistrict and fp (fireplace status)

```{r}
xtabs(~ hsdistrict + fp, data = d)
```


Notice missing values are dropped by default! Set `addNA = TRUE` to see missing as a separate category

```{r}
xtabs(~ hsdistrict + fp, data = d, addNA = TRUE)
```

We can save a table as an object.

```{r}
tab <- xtabs(~ hsdistrict + fp, data = d)
```

To see the table, simply submit it.

```{r}
tab
```


Portions of table can be extracted using indexing brackets

```{r}
# row 1
tab[1,]
```


```{r}
# column 1
tab[,1]
```

```{r}
# row 1, column 2
tab[1,2]
```

Adding `drop=FALSE` preserves table structure

```{r}
tab[1, , drop = FALSE]
```

```{r}
tab[, 1, drop = FALSE]
```

```{r}
tab[1, 2, drop = FALSE]
```

We can also use row and column names to extract rows/columns/cells

```{r}
tab["Albemarle", ]
```

```{r}
tab[, "1"]
```

```{r}
tab["Albemarle", "1"]
```


Given high school district, what proportion of homes do and do not have fireplaces? Condition on rows by setting `margin = 1`. (row proportions sum to 1)

```{r}
tab %>% 
  proportions(margin = 1) %>% 
  round(2)
```


Given homes with and without fireplaces, what proportions are in each high school district? Condition on columns by setting `margin = 2` (column proportions sum to 1)

```{r}
tab %>% 
  proportions(margin = 2) %>% 
  round(2)
```

Let's save table of proportions so we can do further work.

```{r}
tab_p <- tab %>% 
  proportions(margin = 1) %>% 
  round(2)
tab_p
```


What's the difference in proportion of homes with a fireplace between Albemarle and Monticello? 

```{r}
c(tab_p["Albemarle", "1"],
  tab_p["Monticello", "1"])
```


Absolute difference = 0.16 or 16%

```{r}
tab_p["Albemarle", "1"] - tab_p["Monticello", "1"]
```

The difference in proportions is 0.16. The proportion of homes with fireplaces
in Albemarle is 0.16 higher than the proportion of homes with fireplaces in
Monticello.

Relative difference = 1.25 or 25% 

```{r}
tab_p["Albemarle", "1"]/tab_p["Monticello", "1"]
```


The ratio of proportions is 1.25. Albemarle has about 25% more homes with fireplaces than Monticello.

### 3-way tables (and beyond)

We can also go beyond two dimensions. This is sometimes called "stratifying" on additional variables.

Cross-tabulation of hsdistrict and fp (fireplace status) _stratified by cooling status_: `~ hsdistrict + fp + cooling` (The last variable will be the stratifying variable, or 3rd dimension)

```{r}
tab2 <- xtabs(~ hsdistrict + fp + cooling, data = d)
tab2
```

Portions of table can be extracted with indexing brackets; need to use 3 dimensions.

The third dimension refers to the strata

```{r}
# all rows, all columns, 1st dimension: cooling = No Central Air
tab2[,,1] 
```

```{r}
# all rows, all columns, 2nd dimension: cooling = Central Air
tab2[,,2] 
```

Or using names

```{r}
tab2[,,"No Central Air"]
```

```{r}
tab2[,,"Central Air"]  
```


Adding `drop = FALSE` preserves row/column/strata names. This is useful when working with 3 or dimensions.

```{r}
tab2[, , "Central Air", drop = FALSE]
```


```{r}
# Monticello, all columns, all strata
tab2["Monticello", , , drop = FALSE]
```

```{r}
# all rows, fp = "1", all strata 
tab2[, "1", ,drop = FALSE]
```


Calculate proportions across rows within each strata.

Within each cooling strata, for each high school district, what proportion of homes have and do not have fireplaces? Set `margin = c(1, 3)` (ie, across rows, within each strata)

```{r}
tab2 %>% 
  proportions(margin = c(1, 3)) %>% 
  round(2)
```


Within each cooling strata, for homes without and with fireplaces, what proportion are in each high school district? Set `margin = c(2, 3)` (ie, down columns, within each strata)

```{r}
tab2 %>% 
  proportions(margin = c(2, 3)) %>% 
  round(2)
```

For homes with central air, Monticello has 0.32 without a fireplace versus 0.19 for Albemarle. 

```{r}
tab2 %>% 
  proportions(margin = c(1, 3)) %>% 
  round(2)
```


What's the absolute and relative differences?

```{r}
# save table
tab2_p <- tab2 %>% 
  proportions(margin = c(1, 3)) %>% 
  round(2)

```


Absolute difference:

```{r}
tab2_p["Monticello", "0", "Central Air"] - 
  tab2_p["Albemarle", "0", "Central Air"]
```


Of all homes with central air, the proportion of homes without a fireplace in Monticello is 0.13 higher than the proportion of homes with a fireplace in Albemarle.

Relative difference:

```{r}
tab2_p["Monticello", "0", "Central Air"]/
  tab2_p["Albemarle", "0", "Central Air"]
```


Of all homes with central air, Monticello has about 68% more homes without a fireplace than Albemarle.

Can also calculate counts and proportions based on a condition.

Proportion of homes over 2000 finsqft in size:

```{r}
mean(d$finsqft > 2000)
```

Proportion of homes with 3 or 4 bedrooms:

```{r}
mean(d$bedroom %in% 3:4)

```


These conditions can be included in a call to `xtabs()`

```{r}
xtabs(~ (finsqft > 2000) + hsdistrict, data = d) %>% 
  proportions(margin = 2) %>% 
  round(2)
```

The tidyverse way for counts and proportions requires more work and is documented in an appendix at the end of this script.

## CODE ALONG 1

(1) Compare the absolute and relative proportions of homes with no central air in the Burley and Walton middle school districts.

```{r}
tp <- xtabs(~ msdistrict + cooling, data = d) %>% 
  proportions(margin = 1) %>% 
  round(3)
tp

# absolute
tp["Burley", "No Central Air"] - tp["Walton", "No Central Air"]

# relative
tp["Burley", "No Central Air"]/tp["Walton", "No Central Air"]

```


(2) What proportion of homes are on more than 1 acre of land (lotsize) 

```{r}
mean(d$lotsize > 1)
```


(3) What proportion of homes are on more than 1 acre of land (lotsize) within each hsdistrict?

```{r}
xtabs(~ (lotsize > 1) + hsdistrict, data = d) %>% 
  proportions(margin = 2) %>% 
  round(2)
```


## Summarizing numeric data

`summary()` and `hist()` make a powerful combination for summarizing one variable. Let's investigate the lotsize variable.

```{r}
summary(d$lotsize)
```

```{r}
hist(d$lotsize)
```

We can use the `breaks` argument to increase/decrease number of bins.

```{r}
hist(d$lotsize, breaks = 100)
```


The standard deviation and IQR are wildly different. If we had to pick one to summarize the spread, we would probably choose IQR since the data is not symmetric.

```{r}
sd(d$lotsize)
```

```{r}
IQR(d$lotsize)
```

Summarizing discrete numeric data presents unique challenges. Sometimes the mean is nonsensical.

```{r}
summary(d$bedroom)
```

Histograms can hide the discreteness.

```{r}
hist(d$bedroom)
```


Discrete data may be better visualized with a bar plot, created by first tabling the data.

```{r}
table(d$bedroom) %>% plot()
```

ggplot makes it pretty easy to add axis tick mark for 11

```{r}
ggplot(d) +
  aes(x = bedroom) +
  geom_bar() +
  scale_x_continuous(breaks = 0:12, minor_breaks = FALSE)
```

### quantiles/percentiles

Given the quantile/percentile, what's the value? What is 75th percentile of home values?

```{r}
quantile(d$totalvalue, probs = 0.75)
```

The default returns min/max and quartiles.

```{r}
quantile(d$totalvalue)
```

Here's a trick to get deciles. Set `probs = 1:9/10`

```{r}
quantile(d$totalvalue, probs = 1:9/10)
```


### Empirical cumulative distribution (ECD)

The ECD is the inverse of quantile.

Given the value, what's the quantile/percentile? What percentile is a $500,000 home in? Use the `ecdf()` function to create a function from the data to return quantiles.

```{r}
Fn <- ecdf(d$totalvalue)
Fn(500000) 
```

What percentile is a $1,000,000 home in?

```{r}
Fn(1e6)   
```

We can also plot the ECD

```{r}
plot(Fn)
```


### Numeric summaries by group

We often to calculate summaries by groups. For example, what is the mean totalvalue by high school district?

The `aggregate()` function can help us do this. It returns a data frame and uses the same formula syntax as `xtabs()`. Place the numeric variable on the left hand side. The last argument is the function we wish to use to summarize the numeric data in each group.

```{r}
aggregate(totalvalue ~ hsdistrict, data = d, mean)
```

```{r}
aggregate(totalvalue ~ hsdistrict, data = d, median)
```

```{r}
aggregate(totalvalue ~ hsdistrict, data = d, IQR)
```

We can also define our own functions. Below we calculate the proportion of homes valued at over 1 million dollars in each high school district.

```{r}
aggregate(totalvalue ~ hsdistrict, data = d, function(x)mean(x > 1e6)) 

```

The `tapply()` returns a vector or list and requires vectors.

```{r}
tapply(d$totalvalue, d$hsdistrict, mean)
```

```{r}
tapply(d$totalvalue, d$hsdistrict, median)
```

```{r}
tapply(d$totalvalue, d$hsdistrict, IQR)
```

`tapply()` is ideal for use with functions that return lots of values. For example, using the `summary()` for totalvalue within each high school district.

```{r}
tapply(d$totalvalue, d$hsdistrict, summary)  # list
```

```{r}
tapply(d$totalvalue, d$hsdistrict, quantile)  # list
```


We can also include 2 or more groups with `tapply()` and `aggregate()`

For `aggregate()`, separate groups by `+`. The result is a data frame

```{r}
aggregate(totalvalue ~ hsdistrict + cooling, data = d, median)
```

For `tapply()`, wrap groups in `list()`. The result is a matrix.

```{r}
tapply(d$totalvalue, list(d$hsdistrict, d$cooling), mean)
```

Incidentally, the {car} package provides the function `Tapply()` that allows one to use a formula interface as they would with `aggregate()`

```{r}
car::Tapply(totalvalue ~ + hsdistrict + cooling, data = d, mean)
```

A matrix is desirable for _reporting_.
A data frame is desirable for _plotting_. 

For example, notice how we can directly pipe the output of `aggregate()`, a data frame, into ggplot code for for plotting.

```{r}
aggregate(totalvalue ~ hsdistrict + cooling, data = d, median) %>% 
  ggplot() +
  aes(x = hsdistrict, y = totalvalue, color = cooling) +
  geom_point(size = 3) +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = 'Median home values by HS District and Central Air status')

```

Some R users prefer using tidyverse functions for this type of summary, but notice the missing values are included by default in the legend.

```{r}
d %>% 
  group_by(hsdistrict, cooling) %>% 
  summarize(totalvalue = median(totalvalue)) %>% 
  ggplot() +
  aes(x = hsdistrict, y = totalvalue, color = cooling) +
  geom_point(size = 3) +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = 'Median home values by HS District and Central Air status')
```

To drop the missing values we need to filter them out. Below `!is.na()` means filter out the records that are not missing a value for cooling.

```{r}
d %>% 
  filter(!is.na(cooling)) %>% 
  group_by(hsdistrict, cooling) %>% 
  summarize(totalvalue = median(totalvalue)) %>% 
  ggplot() +
  aes(x = hsdistrict, y = totalvalue, color = cooling) +
  geom_point(size = 3) +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = 'Median home values by HS District and Central Air status')
```


More summaries are available in other R packages; here are two that may be of interest. Notice we can reference functions from other packages without loading the packages using the `package::function` notation.

```{r}
# describe() from the psych package
d %>% 
  select(totalvalue, finsqft, lotsize) %>% 
  psych::describe()
```

```{r}
# describe() from the Hmisc package
d %>% 
  select(totalvalue, finsqft, lotsize) %>% 
  Hmisc::describe()
```


### Correlation

Summarize linear relationship between finsqft and totalvalue.

```{r}
cor(d$finsqft, d$totalvalue, method = "pearson")
```

```{r}
cor(d$finsqft, d$totalvalue, method = "spearman")
```

Correlation should always be accompanied by a scatter plot. A smooth trend line can help us assess the nature of the linear relationship.(if any exists)

```{r}
ggplot(d) +
  aes(x = finsqft, y = totalvalue) +
  geom_point() +
  geom_smooth()

```


correlation between bedrooms and full bathrooms

```{r}
cor(d$bedroom, d$fullbath) # Why NA?
```

We have missing data

```{r}
summary(d$bedroom)
```

```{r}
summary(d$fullbath)
```

When data is missing, we can specify `use = "complete.obs"`

```{r}
cor(d$bedroom, d$fullbath, use = "complete.obs")
```


It's worth noting this is very discrete data

```{r}
plot(bedroom ~ fullbath, data = d)
```

Jittering can help reveal where most of the data is located.

```{r}
ggplot(d) +
  aes(x = fullbath, y = bedroom) +
  geom_jitter(alpha = 1/5) +
  scale_x_continuous(breaks = 0:11, minor_breaks = FALSE) +
  scale_y_continuous(breaks = 0:12, minor_breaks = FALSE)
```


The `cor()` function works for data frames. This allows us to calculate correlation matrices. Below we select three columns and then pipe into the `cor()` function.  

```{r}
d %>% 
  select(totalvalue, finsqft, lotsize) %>% 
  cor()
```

As always, we should visualize the data. Here's a quick way to visualize all pairwise scatterplots using the base R `pairs()` function.

```{r}
d %>% 
  select(totalvalue, finsqft, lotsize) %>% 
  pairs()
```


### Log transformations


The idea: Take positive, skewed data and hopefully make it more symmetric.

Values clumped together get spread apart, values far away are moved closer to the rest of the data.

```{r}
hist(d$totalvalue)
```

log transformed histogram

```{r}
hist(log(d$totalvalue))
```


To undo a log transformation we take the "anti-log", or exponentiate using the `exp()` function.

```{r}
x <- 1200000
logx <- log(x)
logx
exp(logx) # the anti-log
```

Mean of untransformed data

```{r}
mean(d$totalvalue)
```

Mean of log transformed data

```{r}
mean(log(d$totalvalue))
```

When we take the anti-log of the mean of the log-transformed values, we get the "geometric mean". Notice this mean is closer to the median (407,000) and is less affected by the outliers.

```{r}
exp(mean(log(d$totalvalue)))
```


## CODE ALONG 2 

(1) summarize finsqft numerically and visually

```{r}
summary(d$finsqft)
hist(d$finsqft)
```


(2) What is the 90th percentile of finsqft? In what percentile is 1500 finsqft?

```{r}
quantile(d$finsqft, probs = 0.90)
Fsq <- ecdf(d$finsqft)
Fsq(1500)
```



(3) what is the correlation between finsqft and totalrooms (presumably they're correlated)? Compare the correlation to a scatter plot between the two variables.

```{r}
cor(d$finsqft, d$totalrooms, use = "complete.obs")
cor(d$finsqft, d$totalrooms, use = "complete.obs", method = "spearman")

```


```{r}
ggplot(d) +
  aes(x = totalrooms, y = finsqft) +
  geom_point() +
  geom_smooth() +
  xlim(0, 20)

```


(4) summarize the log-transformed improvements value. What do we notice?

```{r}
summary(d$improvementsvalue)
hist(d$improvementsvalue, breaks = 200)
summary(log(d$improvementsvalue))
summary(log(d$improvementsvalue[d$improvementsvalue > 0]))

d %>% 
  filter(improvementsvalue > 0) %>% 
  summarise(m = exp(mean(log(improvementsvalue))))

```

## Uncertainty 

### Uncertainty about a mean

Let's pretend for a moment we don't have all the data for Albemarle County homes. Instead, we can only sample 50 homes. And we have to use that sample to make our best estimate of the mean totalvalue of homes in Albemarle County.

We can use the `sample()` function to randomly sample elements of a vector. If you run this code will get a different sample than I get. Then we take the mean of our sample.

```{r}
samp <- sample(d$totalvalue, 50)
mean(samp)
```

We will all get different means. How uncertain are our estimates of the population mean? This is one of the fundamental aims of statistical inference: estimating the _standard error_ of an estimate. 

A theoretical estimate of the standard error of the sample means is to take the standard deviation of the sample and divide by the square root of the sample size.

```{r}
sd(samp)/sqrt(50)  # standard error of the mean
```

Again, if you're following along in your version of this notebook, you got a different standard error.

Our estimate of the population mean is our sample mean give or take the standard error. To be on the safe side, we usually give or take about two standard errors.

R allows us to easily simulate the process of sampling data and calculating the mean. Below we use the `replicate()` function to replicate our code above 10,000 times.


```{r}
means <- replicate(n = 10000, expr = {
  samp <- sample(d$totalvalue, 50)
  mean(samp)
})
```

The standard deviation of our 10,000 sample means is the estimated standard error. In theory, standard error is the _standard deviation of the sample means_.

```{r}
sd(means) # standard error based on 10,000 means
```

We can add and subtract 2 standard errors from mean to form an approximate 95% confidence interval

```{r}
SE_m <- sd(means)
mean(samp) + c(-1, 1)*2*SE_m
```

The above was for educational purposes. In practice, we don't get to take 10,000 random samples. Instead we would take our single sample and estimate a confidence interval using a math formula courtesy of a function such as `t.test()`

```{r}
t.test(samp)
t.test(samp)$conf.int

```

The {Hmisc} package provides the `smean.cl.normal()` function that returns the mean and the lower/upper limits.

```{r}
Hmisc::smean.cl.normal(samp)
```

Again, if you're running this code, you're getting a different answer because we all took a different sample.

As our samples get bigger, our standard error decreases. We have more information, therefore there is less uncertainty in our estimate.

NOTE: To reduce the standard error by 1/2, we need 4 times the sample size. So doubling our sample size of 50 to 100 _does not_ reduce our uncertainty by half. We would need a sample size of 50*4 = 200.


### Uncertainty about a proportion

Let's take a random sample of 50 homes' fireplace status. The fp variable is a sequence of zeroes and ones, where a one means a home has at least one fireplace. Taking the mean of zeroes and ones returns the proportion of ones. We use `na.rm = TRUE` in out `mean()` function because some homes are missing this information in our data.

```{r}
samp2 <- sample(d$fp, 50)
mean(samp2, na.rm = TRUE)
```

We should all get a different answer due to the sampling variability.

Let's repeat that code 10,000 times.

```{r}
props <- replicate(n = 10000, expr = {
  samp2 <- sample(d$fp, 50)
  mean(samp2, na.rm = TRUE)
})
sd(props) # standard error based on 10,000 means
```

We can add and subtract 2 standard errors from the proportion to form an approximate 95% confidence interval.

```{r}
SE_p <- sd(props)
mean(samp2, na.rm = TRUE) + c(-1, 1)*2*SE_p
```

The above was for educational purposes. In practice, we don't get to take 10,000 random samples. Instead we would take our single sample and estimate a confidence interval using a math formula courtesy of a function such as `prop.test()`. The first argument `x` in `prop.test()` is the number of successes (or homes with fireplaces.) The second argument `n` is the size of our sample.

```{r}
prop.test(x = sum(samp2, na.rm = TRUE), n = 50)
prop.test(x = sum(samp2, na.rm = TRUE), n = 50)$conf.int
```

The {binom} package provides the `binom.confint()` function that returns the proportion and the lower/upper limits of 11 different intervals (!). Different intervals may be preferred when your estimated proportion is near 0 or 1 and/or your sample size is small.

```{r}
binom::binom.confint(x = sum(samp2, na.rm = TRUE), n = 50)
```


### 95% Confidence intervals

The _process_ of calculating a confidence interval works 95% of the time. The "95%" is due to the fact that about 95% of the probability under a normal distribution is within two standard errors of the mean. And most sampling distributions are approximately normal.

Imagine a population that is truly massive, trillions in size, virtually infinite, normally distributed with a mean of 10 and a standard deviation of 2. Here's a picture of the distribution of that population:

```{r}
# dnorm() is the formula to draw the Normal dist'n
curve(dnorm(x, mean = 10, sd = 2,), from = (10 - 6), to = (10 + 6), 
      main = "distribution of huge population", ylab = "", xlab = "")
```

Let's pretend to sample 30 values from this population. We can do this with the `rnorm()` function.

```{r}
z <- rnorm(30, mean = 10, sd = 2)
```

Now calculate confidence interval on estimated mean using the `t.test()` function.

```{r}
tout <- t.test(z)
tout$conf.int
```

Is true value of 10 contained in confidence interval?

```{r}
tout$conf.int[1] < 10 & tout$conf.int[2] > 10
```


Repeat the process 1000 times using `replicate()`:

```{r}
ci_test <- replicate(n = 1000, expr = {
  z <- rnorm(30, mean = 10, sd = 2)
  tout <- t.test(z)
  tout$conf.int[1] < 10 & tout$conf.int[2] > 10
})

```

The result is a vector of TRUE/FALSE values, which R interprets as ones (TRUE) and zeroes (FALSE). The mean of ones and zeroes is the proportion of ones, or TRUEs in this case.

Proportion of times 95% Confidence Interval contains 10:

```{r}
mean(ci_test)
```

We just demonstrated the idea of the confidence interval. We are confident in the process. 

Now let's try it with the _very skewed_ homes total values. First we'll calculate the "true" mean since we have virtually all of the data.

```{r}
true_mean <- mean(d$totalvalue)
true_mean
```

Next we take a sample of size 50 and check if the 95% confidence interval contains the "true" mean:

```{r}
samp <- sample(d$totalvalue, 50)
tout <- t.test(samp)
tout$conf.int[1] < true_mean & tout$conf.int[2] > true_mean
```

Now we repeat the process 1000 times and check the proportion of times our 95% CI contains the true mean:

```{r}
ci_test <- replicate(n = 1000, expr = {
  samp <- sample(d$totalvalue, 50, replace = TRUE)
  tout <- t.test(samp)
  tout$conf.int[1] < true_mean & tout$conf.int[2] > true_mean
})
mean(ci_test)

```

The coverage is actually not 95%! This can happen when sampling from very skewed populations.

Let's try log-transforming our sample.

```{r}
true_log_mean <- mean(log(d$totalvalue))
ci_test_log <- replicate(n = 1000, expr = {
  samp <- sample(d$totalvalue, 50, replace = TRUE)
  tout <- t.test(log(samp))
  tout$conf.int[1] < true_log_mean & tout$conf.int[2] > true_log_mean
})
mean(ci_test_log)

```

This has the stated 95% coverage. We might say we're confident in the process of calculating confidence intervals for log-transformed data.

Again, in practice we only get one sample. We don't get to resample the population thousands of times.

### Bootstrapping

However we can do something called _bootstrapping_ that treats the sample like the population.

Let's take one sample of size 50:

```{r}
tv <- sample(d$totalvalue, 50)
```

Now let's resample from our sample _with replacement_ 1000 times, each time calculating the sample mean.

```{r}
boot_mean <- replicate(n = 1000, expr = {
  tv_samp <- sample(tv, replace = TRUE)
  mean(tv_samp)
})
```

The standard deviation of our bootstrap means can serve as an estimate of _standard error_.

```{r}
sd(boot_mean)
```

Further we can get 95% confidence interval using 2.5% and 97.5% percentiles:

```{r}
quantile(boot_mean, probs = c(0.025, 0.975))
```

The `smean.cl.boot()` function from the {Hmisc} package does all this for us. If you run this too, you'll get a slightly different result.

```{r}
Hmisc::smean.cl.boot(tv)
```


Bootstrapping works well for creating confidence intervals for medians.

```{r}
boot_median <- replicate(n = 1000, expr = {
  tv_samp <- sample(tv, replace = TRUE)
  median(tv_samp)
})

# get 95% CI as 2.5 and 97.5 percentiles
quantile(boot_median, probs = c(0.025, 0.975))
```


## CODE ALONG 3

The following code randomly samples 120 homes in the Western Albemarle High school district. Use this data frame to answer the following questions.

```{r}
set.seed(123)
wa_samp <- d %>% 
  filter(hsdistrict == "Western Albemarle") %>% 
  sample_n(120)
```


(1) Calculate 95% confidence interval on mean finsqft

```{r}
t.test(wa_samp$finsqft)
smean.cl.normal(wa_samp$finsqft)
```


(2) Does the 95% confidence interval capture the true value?

```{r}
d %>% 
  filter(hsdistrict == "Western Albemarle") %>% 
  summarize(mean(finsqft))
```


(3) Use a bootstrap to calculate a 95% confidence interval for the median age of homes in our sample. Use 1000 bootstraps.

```{r}
bootout <- replicate(n = 1000, expr = {
  sage <- sample(wa_samp$age, replace = TRUE)
  median(sage)
})
quantile(bootout, probs = c(0.025, 0.975))
```


(4) Does the 95% bootstrap CI capture the true median age value?

```{r}
d %>% 
  filter(hsdistrict == "Western Albemarle") %>% 
  summarize(median(age))
```


## Hypothesis Testing 

Sometimes researchers do hypothesis testing on "all the data" based on the assumption the data collected is just one sample of a super-population of possibilities.

### Chi-square test of association

Is condition of home associated with high school district? There are small differences in the proportion of average and good homes.

```{r}
xtabs(~ condition + hsdistrict, data = d) |>
  proportions(margin = 2) |>
  round(2)
```

We can use a Chi-square test of association to investigate. This hypothesis test allows us to calculate the probability of getting differences in proportions as big or bigger assuming no difference among the high school districts. The `chisq.test()` function can be used with a table object.

```{r}
ct <- xtabs(~ condition + hsdistrict, data = d)
chisq.test(ct)
```

A small p-value provides evidence against the null of no association. We might conclude that knowing the high school district gives us some information about the expected condition of a house.

But it's important to note the p-value does not tell us anything about how conditions differ between high school districts.

_Residuals_ tell us which cell counts are bigger/smaller than expected. Values greater than 2.5 or 3 in absolute value are of interest. These are standardized values.

```{r}
ctest <- chisq.test(ct)
ctest$residuals
```

It appears there are more homes in "Good" condition in Albemarle county than expected, and fewer homes in "Good" condition in Monticello then expected.

### two-sample proportion test

Is the difference in proportions of Excellent homes in Albemarle and Western Albemarle "significant"? (Many statisticians hate that phrase)

```{r}
xtabs(~ condition + hsdistrict, data = d) |>
  proportions(margin = 2) |>
  round(3)
```

To compare two proportions we can use the `prop.test()` function. We need the number of homes that rated "Excellent" and the total number of homes. The `addmargins()` function gives us marginal totals (ie, row sums and column sums)

```{r}
xtabs(~ condition + hsdistrict, data = d) %>% 
  addmargins()
```

We can run the hypothesis test using `prop.test()`. The first argument is number of homes in Excellent condition for Albemarle and Western Albemarle, respectively. The second argument is the total number of homes in each high school district.

```{r}
prop.test(x = c(189, 239), n = c(12714, 9421))
```

Statistically it seems significant with such a small p-value, but practically speaking the difference doesn't seem that dramatic. Big sample sizes can lead to "significant" p-values for very small differences.

### two sample t-test

Does totalvalue differ between homes with and without fireplaces? Could investigate with a t-test. We can use R's formula syntax `totalvalue ~ fp` that basically says in this case "compare totalvalue grouped by fp"

```{r}
t.test(totalvalue ~ fp, data = d)
```


A small p-value provides evidence against the null of no difference, but does not tell us about the difference. Instead look at 95% confidence interval on the difference.

The formal null hypothesis of a two-sample t-test is that both samples came from the same normal distribution. 

Maybe we should look at the distribution of totalvalue by fp.

```{r}
ggplot(filter(d, !is.na(fp))) +
  aes(x = totalvalue, fill = factor(fp)) +
  geom_density(alpha = 1/3)
```


Ideally we would like to see the distributions of each group looking roughly symmetric. One option is a log transformation

```{r}
ggplot(filter(d, !is.na(fp))) +
  aes(x = log(totalvalue), fill = factor(fp)) +
  geom_density(alpha = 1/3)

```

Let's try a t-test on the log-transformed data.

```{r}
tout <- t.test(log(totalvalue) ~ fp, data = d)
tout

```

We get the same basic result as far as the hypothesis test goes, but the estimated means and confidence interval are on the log scale.

We can use the `exp()` function to undo the log transformation (ie, take the anti-log) and see the expected mean values, aka the geometric means.

```{r}
exp(tout$estimate)
```


The confidence limits for the difference between means cannot be transformed back to the original scale

```{r}
tout$conf.int
```

Instead, if we exponentiate, we get a 95% CI for the _ratio of the geometric means_. The geometric mean of homes without fireplaces is about 44% - 43% lower than homes with fireplaces. (ie, \$277,741 is about 43-44% lower than \$491,747)

```{r}
exp(tout$conf.int)
```


```{r}
1 - exp(tout$conf.int) # about 44% - 43% lower
```

We can calculate the ratio of the geometric means. It's about 0.56. The 95% CI
is about [0.55, 0.57]

```{r}
exp(12.53444)/exp(13.10572)
```

Again this demonstrates how a log-transformation results in an analysis of relative differences instead of absolute differences. 

### Wilcoxon Rank Sum test (aka Mann-Whitney)

Instead of a log transformation we might have considered a non-parametric test, such as the _Wilcoxon Rank Sum test_. Whereas the formal null hypothesis of a two-sample t-test is that both samples came from the same normal distribution, the formal null of the Wilcoxon is that both samples came from the same distribution. No particular distribution is assumed. 

This Wilcoxon Rank Sum test compares the _ranks_ of the data instead of the values themselves. We can do this with the `wilcox.test()` function. This takes a second!

```{r}
wilcox.test(totalvalue ~ fp, data = d, conf.int = TRUE)
```

The "difference in location" estimates the median of the difference between a sample from homes with fireplaces and a sample from homes without fireplaces.

Again, given how much data we have and how big the difference is between mean totalvalues for homes without and with fireplaces, the p-value of whatever test we choose seems to be a formality not terribly informative.

## CODE ALONG 4

Observe the following proportions

```{r}
xtabs(~ hsdistrict + fp, data = d) %>% 
  proportions(margin = 1) %>% 
  round(2)
```


(1) Assuming no difference in proportion of fireplaces in Albemarle and Western Albemarle high school districts, what is probability we would get data like this or more extreme (0.79 vs 0.75)?

```{r}
xtabs(~ hsdistrict + fp, data = d) %>% 
  addmargins()
prop.test(x = c(9962, 7014), n = c(12615, 9382))
```


## Modeling 

Look at the distribution of total values.

Obviously there is a great deal of variability in the value of a home. What might explain that variability?

```{r}
hist(d$totalvalue, breaks = 40)
```


Notice some of that variability appears to be associated with finsqft

```{r}
plot(totalvalue ~ finsqft, data = d)
```


We can attempt to model the totalvalue of a home using finsqft. In other words, create a a math formula that allows us to estimate the expected totalvalue of a home based on its finsqft.

### MODEL 1: simple linear model (one predictor)

Model totalvalue as a function of finsqft using the `lm()` function with syntax `totalvalue ~ finsqft`, and save to an object called "m1"

```{r}
m1 <- lm(totalvalue ~ finsqft, data = d)
summary(m1)

```

e+05 is scientific notation; it means move the decimal 5 places to the right.

We can view the coefficients of a model using the `coef()` function. Sometimes this helps with the scientific notation.

```{r}
coef(m1)
```

The fitted model:

```
totalvalue = -164163.6587 + 314.6012*finsqft + ERROR
```
where ERROR is a random draw from a Normal dist with mean = 0 and sd = 259300. Predicted totalvalue using the model is expected to be off by about \$259,300.

_Interpretation:_
Each additional finsqft adds about \$314 to value of home. This is an _absolute_ effect. Whether you have 1500 finsqft or 2500 finsqft, we can expect each additional finsqft to add about \$314 to the totalvalue.

The `confint()` function reports 95% confidence intervals on the model coefficients.

```{r}
confint(m1)
```

Each additional finsqft adds about 311-317 to the value of home.

Let's use our model to make a prediction.

What is the expected _mean value_ of homes with 2500 finsqft and the associated 95% confidence interval? Use `interval = "confidence"` since we're interested in the expected mean.

```{r}
predict(m1, newdata = data.frame(finsqft = 2500), 
        interval = "confidence")
```


What is the expected _value of a single home_ with 2500 finsqft and the associated 95% confidence interval? Use `interval = "prediction"` since we're interested in a predicted value.

```{r}
predict(m1, newdata = data.frame(finsqft = 2500), 
        interval = "prediction")
```

Notice how much wider this confidence interval is. That's because we're more uncertain about predicting a specific value than predicting a mean.

But is this a good model? Should we trust these predictions?

Adjusted R-squared:  0.578 
This suggests about 58% of the variation in totalvalue is "explained" by finsqft.

Can also look at residuals versus fitted value plot
residual = observed value - fitted value

There are 6 available plots; we ask for the first plot

```{r}
plot(m1, which = 1)
```

The dotted line is 0. Values above the line have higher totalvalues than the model predicts. Values below the line have lower totalvalues than the model predicts. Ideally we would like to see these points hover steadily around 0 throughout the range of predicted values.

Verdict: Not great, but perhaps somewhat useful. Predicted values seem to get worse as the predicted values get bigger. 

### MODEL 2: log transformed response

When working with dollar amounts it is common to log transform the values. Dollar amounts, especially for things like properties and salaries, can be skewed. That's often because they scale multiplicatively as they get larger. (Salaries don't go up by $1000. They go up by 3%.)

Let's model log-transformed totalvalue as a function of finsqft. This changes how we interpret the finsqft coefficient

```{r}
m2 <- lm(log(totalvalue) ~ finsqft, data = d)
summary(m2)
```

We can get a better sense of the finsqft coefficient by multiplying by 100.

```{r}
coef(m2)["finsqft"] * 100
```

Every additional 100 finsqft adds about 4.8% to totalvalue of home. This is a _relative_ effect. The 4.8% change is constant, but relative to the current totalvalue.

Is this a good model? The residual vs fitted plot is a good first check.

```{r}
plot(m2, which = 1)
```


Verdict: Now predicted values seem worse for smaller predicted values. 

### MODEL 3: non-linear effect of predictor

In the previous two models we assumed the effect of finsqft was _linear_ (ie, it had a constant slope). Going from 1500 - 1600 finsqft has the same effect on totalvalue as going from 2500 - 2600 finsqft.

It may be more realistic to assume a _non-linear_ effect. This is, an effect that is not constant. Maybe adding more finsqft to a home has a more dramatic impact on totalvalue when the home has around 1500 finsqft versus a home that is 3500 finsqft.

We can model non-linear effects using _natural splines_. The {splines} package that comes with R makes this relatively easy to implement. Splines are more sophisticated and better behaved than polynomials.

The `ns(finsqft, df = 2)` code is similar to fitting a polynomial: x + x^2

```{r}
library(splines)
m3 <- lm(log(totalvalue) ~ ns(finsqft, df = 2), data = d)
summary(m3)
```

There is no interpretation of coefficients! The effect is not a simple coefficient. Before we worry about interpretation, is this a good model?

```{r}
plot(m3, which = 1)
```

Verdict: This seems like an improvement on model 2

Let's visualize and use the model. We use the `ggpredict()` function from the {ggeffects} package to create predictions at various finsqft values and then use the {ggeffects} `plot()` method to visualize.

```{r}
ggpredict(m3, terms = "finsqft") %>% 
  plot()
```

We can plot this line with the raw data by adding `add.data = TRUE` to `plot()`.The `dot.alpha = 0.1` argument changes the transparency of the dots

```{r}
ggpredict(m3, terms = "finsqft") %>% 
  plot(add.data = TRUE, dot.alpha = 0.1)
```


Use `predict()` to make model predictions; need to exponentiate to get predicted value on original scale.

```{r}
predict(m3, newdata = data.frame(finsqft = 2500)) %>% exp()
```

Expected mean value of homes with 2500 finsqft

```{r}
predict(m3, newdata = data.frame(finsqft = 2500), 
        interval = "confidence") %>% exp()

```

Expected value of a home with 2500 finsqft

```{r}
predict(m3, newdata = data.frame(finsqft = 2500), 
        interval = "prediction") %>% exp()

```

`ggpredict()` makes it easy to get multiple predictions and 95% CIs. The CIs are for expected mean values.

```{r}
ggpredict(m3, terms = "finsqft[1500:3000 by=500]") 
```


### MODEL 4: three predictors

Let's build a more complex model with multiple predictors, including finsqft, cooling, and fullbath.

Notice we allow finsqft to interact with cooling. We define an interaction with a colon `:`. This says the association between totalvalue and finsqft depends on cooling.

```{r}
m4 <- lm(log(totalvalue) ~ ns(finsqft, df = 2) + cooling + 
           ns(finsqft, df = 2):cooling + 
           fullbath, 
         data = d)
summary(m4)
```


An Analysis of Variance Table can help us assess whether or not interactions are warranted. 

```{r}
anova(m4)
```


The first line is a hypothesis test comparing a model with no predictors (just the overall mean) to a model with `ns(finsqft, df = 2)`. The null is no difference between the models. This is soundly rejected.

The second line is a hypothesis test comparing a model with `ns(finsqft, df = 2)` to a model with `ns(finsqft, df = 2)` and `cooling`. The null is no difference between models. This is also soundly rejected. And so on down the table.

The last line tests whether the interaction should be included. Based on the p-value it seems warranted.

This seems like an OK model, at least for most homes. The residual versus fitted value plot seems reasonable.

```{r}
plot(m4, which = 1)
```


Let's visualize the interaction in the model using effect plots. We can do that with `ggpredict()` by specifying `terms = c("finsqft", "cooling")`.

The interaction appears to happen after 5000 finsqft. But the thick confidence ribbon indicates there are few (if any) homes that big without cooling.

```{r}
ggpredict(m4, terms = c("finsqft", "cooling")) %>% 
  plot()
```


The "significant" interaction doesn't seem that interesting when zoomed in on a reasonable range of finsqft. The syntax `finsqft[1000:5000]` only works for functions in the {ggeffects} package.

```{r}
ggpredict(m4, terms = c("finsqft[1000:5000]", "cooling")) %>% 
  plot()
```


We can view the effect of fullbaths from 1 - 5.


```{r}
ggpredict(m4, terms = "fullbath[1:5]") %>% 
  plot()
```


Notice this plot is for cooling = No Central Air and finsqft = 1936.

```{r}
ggpredict(m4, terms = "fullbath[1:5]")
```


We can manually set the "conditional" values using the condition argument; notice the shape doesn't change, just the y-axis.

```{r}
ggpredict(m4, terms = "fullbath[1:5]", 
          condition = list(cooling = "Central Air",
                           finsqft = 2000)) %>% 
  plot()
```


Many hypothesis tests are special cases of linear models

ANOVA (one categorical predictor)

```{r}
m5 <- lm(totalvalue ~ hsdistrict, data = d)
anova(m5)
ggpredict(m5, terms = "hsdistrict") %>% plot()

```


t-test (one binary predictor)

```{r}
m6 <- lm(log(totalvalue) ~ cooling, data = d)
summary(m6)
ggpredict(m6, terms = "cooling") %>% plot()

```


2-sample proportion test (logistic regression)

```{r}
m7 <- glm(fp ~ hsdistrict, data = d, family = binomial)
summary(m7)
ggpredict(m7, terms = "hsdistrict") %>% 
  plot()

```


There are many big books on linear modeling. This was not comprehensive!

## CODE ALONG 5 

(1) Model log(totalvalue) as a function of finsqft, hsdistrict, and their interaction using `lm()`. Call the model "mod1". Visualize the interaction between finsqft and hsdistrict.

```{r}
mod1 <- lm(log(totalvalue) ~ finsqft + hsdistrict + finsqft:hsdistrict, data = d)
anova(mod1)

ggpredict(mod1, terms = c("finsqft[1000:4000 by = 200]", "hsdistrict")) %>% 
  plot()
```



(2) Model "remodel" as a function of hsdistrict using `glm()`. Name the model mod2. In other words, model the probability a home has been remodeled based on the high school district it is located in. Visualize the model.

```{r}
mod2 <- glm(remodel ~ hsdistrict, data = d, family = binomial)
summary(mod2)
ggpredict(mod2, terms = "hsdistrict")
ggpredict(mod2, terms = "hsdistrict") %>% plot()

```





## Appendix: bootstrap CI coverage 

How often does a bootstrap CI capture the true mean?

```{r}
# install the pbapply package so we can have progress bar
install.packages("pbapply")
ci_test_boot <- pbapply::pbreplicate(n = 1000, expr = {
  boot_mean <- replicate(n = 1000, expr = {
    tv_samp <- sample(tv, replace = TRUE)
    mean(tv_samp)
  })
  boot_ci <- quantile(boot_mean, probs = c(0.025, 0.975))
  boot_ci[1] < true_mean & boot_ci[2] > true_mean
})
mean(ci_test_boot)
```


